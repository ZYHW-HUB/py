import os
import pymysql
import psutil
from collections import OrderedDict
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import torchvision.transforms as transforms
from torch.optim.lr_scheduler import StepLR
import json
from torch.cuda.amp import autocast, GradScaler
import multiprocessing
import time
from datetime import timedelta
import csv
from datetime import datetime
from tqdm import tqdm  # æ–°å¢

# --------------------------
# 1. æ•°æ®åº“é…ç½®ä¸æ•°æ®è¯»å–
# --------------------------
db_config = {
    "host": "localhost",
    "port": 3306,
    "user": "root",
    "password": "123",
    "database": "scene",
    "charset": "utf8mb4"
}

# å›¾åƒç›®å½•
image_dir = "walk/deeplabv3-master/training_logs/model_eval_seq"

# æ•°æ®åŠ è½½å‡½æ•°
def load_pair_list(db_config):
    try:
        conn = pymysql.connect(**db_config)
        cursor = conn.cursor()
        query = "SELECT left_id, right_id, winner FROM pp2 WHERE category = 'safety'"
        cursor.execute(query)
        return [(p[0], p[1], p[2]) for p in cursor.fetchall()]
    except pymysql.MySQLError as e:
        print(f"æ•°æ®åº“é”™è¯¯: {e}")
        return []
    finally:
        if 'cursor' in locals(): cursor.close()
        if 'conn' in locals(): conn.close()

# å›¾åƒè·¯å¾„è·å–å‡½æ•°
def get_image_path(image_id):
    return os.path.join(image_dir, f"{image_id}_pred.png")

# åˆ›å»ºè¯„åˆ†è¡¨
def create_scores_table(db_config):
    try:
        conn = pymysql.connect(**db_config)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS image_scores_safety_S (
                image_id VARCHAR(255),
                score FLOAT,
                split VARCHAR(10),
                PRIMARY KEY (image_id, split)
            )
        """)
        conn.commit()
    except pymysql.MySQLError as e:
        print(f"æ•°æ®åº“é”™è¯¯: {e}")
    finally:
        if 'cursor' in locals(): cursor.close()
        if 'conn' in locals(): conn.close()

# æ’å…¥è¯„åˆ†åˆ°æ•°æ®åº“
def insert_scores(db_config, scores, split, conn, cursor, batch_size=1000):
    try:
        query = "INSERT INTO image_scores_safety_S VALUES (%s, %s, %s) ON DUPLICATE KEY UPDATE score=VALUES(score)"
        items = [(str(k), v, split) for k, v in scores.items()]
        for i in range(0, len(items), batch_size):
            cursor.executemany(query, items[i:i+batch_size])
            conn.commit()
    except pymysql.MySQLError as e:
        print(f"æ•°æ®åº“é”™è¯¯: {e.args}")

# --------------------------
# 2. æ™ºèƒ½ç¼“å­˜æ•°æ®é›†
# --------------------------
class SegmentationPairDataset(Dataset):
    def __init__(self, pair_list, transform=None):
        self.pair_list = pair_list
        self.transform = transform
        # æ ‡ç­¾æ˜ å°„: "left" è¡¨ç¤ºå·¦è¾¹è·èƒœï¼Œè½¬æ¢ä¸º 1ï¼›"right" è¡¨ç¤ºå³è¾¹è·èƒœï¼Œè½¬æ¢ä¸º -1ï¼›"equal" è½¬æ¢ä¸º 0
        self.label_map = {"left": 1, "right": -1, "equal": 0}
        self.cache = OrderedDict()
        self.max_cache_size = 800  # åŸºäºå†…å­˜å®¹é‡è°ƒæ•´

    def __len__(self):
        return len(self.pair_list)

    def __getitem__(self, idx):
        if idx in self.cache:
            self.cache.move_to_end(idx)
            return self.cache[idx]
        
        left_id, right_id, winner = self.pair_list[idx]
        left_path = get_image_path(left_id)
        right_path = get_image_path(right_id)

        if not all(os.path.exists(p) for p in [left_path, right_path]):
            raise FileNotFoundError(f"ç¼ºå¤±æ–‡ä»¶: {left_path} æˆ– {right_path}")

        img_left = Image.open(left_path).convert("RGB")
        img_right = Image.open(right_path).convert("RGB")

        if self.transform:
            img_left = self.transform(img_left)
            img_right = self.transform(img_right)

        label = torch.tensor(self.label_map[winner], dtype=torch.float)
        
        if len(self.cache) >= self.max_cache_size:
            self.cache.popitem(last=False)
        self.cache[idx] = (img_left, img_right, label, left_id, right_id)
        
        return self.cache[idx]

# --------------------------
# 3. å†…å­˜ä¼˜åŒ–æ¨¡å‹
# --------------------------
class SiameseNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2)
        )
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Sequential(nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 1))
        
        self._init_weights()
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

    def forward_once(self, x):
        x = x.contiguous(memory_format=torch.channels_last)
        with torch.backends.cuda.sdp_kernel(enable_math=False):
            x = self.conv(x)
            x = self.avgpool(x)
            return self.fc(x.flatten(1))

    def forward(self, x1, x2):
        return self.forward_once(x1), self.forward_once(x2)

# --------------------------
# 4. åŠ¨æ€æŸå¤±è®¡ç®—
# --------------------------
def compute_loss(r1, r2, label, margin=0.5):
    loss = 0.0
    margin_loss = nn.MarginRankingLoss(margin=margin)
    mse_loss = nn.MSELoss()

    mask = label == 1
    if mask.any():
        loss += margin_loss(r1[mask], r2[mask], torch.ones_like(r1[mask]))
    
    mask = label == -1
    if mask.any():
        loss += margin_loss(r2[mask], r1[mask], torch.ones_like(r2[mask]))
    
    mask = label == 0
    if mask.any():
        loss += mse_loss(r1[mask], r2[mask])
    
    return loss

# --------------------------
# 5. è‡ªé€‚åº”ç¡¬ä»¶é…ç½®
# --------------------------
def hardware_config():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åŠ¨æ€å†…å­˜è®¡ç®—
    if device.type == "cuda":
        total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        batch_size = min(64, int(total_mem * 0.7 // 0.25))
    else:
        avail_mem = psutil.virtual_memory().available / 1024**3
        batch_size = min(32, int(avail_mem * 0.6 // 0.15))
    
    grad_accum_steps = 2 if batch_size < 16 else 1
    
    return (
        device,
        max(batch_size, 8),  # æœ€å°æ‰¹æ¬¡å¤§å°
        min(4, os.cpu_count()//2),  # å·¥ä½œè¿›ç¨‹æ•°
        grad_accum_steps
    )

# --------------------------
# 6. è®­ç»ƒæµç¨‹
# --------------------------
if __name__ == '__main__':
    # æ–°å¢æ—¥å¿—ç›®å½•é…ç½®
    log_dir = "walk/deeplabv3-master/training_logs/siamese1"
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"training_log_{datetime.now().strftime('%Y%m%d%H%M')}.csv")
    model_dir = os.path.join(log_dir, "saved_models")
    os.makedirs(model_dir, exist_ok=True)

    # åˆå§‹åŒ–æœ€ä½³æŒ‡æ ‡è·Ÿè¸ªå™¨
    best_metrics = {
        'epoch': 0,
        'val_loss': float('inf'),
        'val_acc': 0.0,
        'train_acc': 0.0
    }

    # åˆ›å»ºCSVæ—¥å¿—æ–‡ä»¶å¤´
    with open(log_file, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow([
            'epoch', 'train_loss', 'val_loss', 
            'train_acc', 'val_acc', 'epoch_time',
            'model_path'
        ])

    multiprocessing.freeze_support()

    # åˆå§‹åŒ–é…ç½®
    device, batch_size, num_workers, grad_accum = hardware_config()
    transform = transforms.Compose([
        transforms.Resize((128, 128)),
        transforms.ToTensor(),
    ])

    # æ•°æ®åŠ è½½
    pair_list = load_pair_list(db_config)
    dataset = SegmentationPairDataset(pair_list, transform)
    train_set, val_set = random_split(dataset, [int(0.8*len(dataset)), len(dataset)-int(0.8*len(dataset))])

    train_loader = DataLoader(
        train_set,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        generator=torch.Generator(device='cpu'),
        persistent_workers=True,
        drop_last=True,
        prefetch_factor=1  # é™ä½é¢„åŠ è½½
    )

    val_loader = DataLoader(
        val_set,
        batch_size=max(batch_size//2, 8),
        shuffle=False,
        num_workers=max(1, num_workers//2),
        pin_memory=True
    )

    # æ¨¡å‹åˆå§‹åŒ–
    model = SiameseNetwork().to(device, memory_format=torch.channels_last)
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    scaler = GradScaler(enabled=torch.cuda.is_available())
    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)
    model_save_path = "walk/deeplabv3-master/training_logs/siamese1/siamese_model.pth"
    scores_save_path = "walk/deeplabv3-master/training_logs/siamese1/scores.json"
    create_scores_table(db_config)

    # æ•°æ®åº“è¿æ¥
    try:
        conn = pymysql.connect(**db_config)
        cursor = conn.cursor()
    except pymysql.MySQLError as e:
        print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        exit(1)

    # è®­ç»ƒå¾ªç¯
    total_start = time.time()
    for epoch in range(10):
        epoch_start = time.time()
        model.train()
        optimizer.zero_grad()
        
        # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡
        running_loss = 0.0
        running_correct = 0  # æ–°å¢è®­ç»ƒæ­£ç¡®æ•°ç»Ÿè®¡
        total_samples = 0    # æ–°å¢è®­ç»ƒæ ·æœ¬æ€»æ•°ç»Ÿè®¡
        train_scores = {}

        # è®­ç»ƒé˜¶æ®µ
        for i, (img_l, img_r, labels, ids_l, ids_r) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}")):  # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            # æ•°æ®ä¼ è¾“ä¼˜åŒ–
            img_l = img_l.to(device, non_blocking=True, memory_format=torch.channels_last)
            img_r = img_r.to(device, non_blocking=True, memory_format=torch.channels_last)
            labels = labels.to(device, non_blocking=True)

            # æ··åˆç²¾åº¦è®­ç»ƒå—
            with autocast(enabled=torch.cuda.is_available()):
                out1, out2 = model(img_l, img_r)
                loss = compute_loss(out1, out2, labels) / grad_accum

                # æ–°å¢å‡†ç¡®ç‡è®¡ç®—ï¼ˆè®­ç»ƒé˜¶æ®µï¼‰
                with torch.no_grad():
                    preds = torch.where(out1 > out2, 1, -1)
                    # å¤„ç†å¹³å±€æ ‡ç­¾ï¼ˆç»Ÿä¸€è½¬æ¢ä¸º1ï¼‰
                    true_labels = labels.clone()
                    true_labels[true_labels == 0] = 1  # ä½¿ç”¨cloneé¿å…ä¿®æ”¹åŸå§‹æ ‡ç­¾
                    correct = (preds.flatten() == true_labels).sum().item()
                    running_correct += correct
                    total_samples += labels.size(0)

            # åå‘ä¼ æ’­
            if torch.cuda.is_available():
                scaler.scale(loss).backward()
                if (i+1) % grad_accum == 0:
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
            else:
                loss.backward()
                if (i+1) % grad_accum == 0:
                    optimizer.step()
                    optimizer.zero_grad()

        # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
        train_acc = running_correct / total_samples if total_samples > 0 else 0.0

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0  # æ–°å¢éªŒè¯æ­£ç¡®æ•°ç»Ÿè®¡
        val_total = 0    # æ–°å¢éªŒè¯æ ·æœ¬æ€»æ•°ç»Ÿè®¡
        val_scores = {}
        
        with torch.inference_mode(), autocast(enabled=torch.cuda.is_available()):
            for img_l, img_r, labels, ids_l, ids_r in tqdm(val_loader, desc="Validation"):  # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
                img_l = img_l.to(device, non_blocking=True, memory_format=torch.channels_last)
                img_r = img_r.to(device, non_blocking=True, memory_format=torch.channels_last)
                labels = labels.to(device, non_blocking=True)
                
                out1, out2 = model(img_l, img_r)
                val_loss += compute_loss(out1, out2, labels).item()

                # æ–°å¢å‡†ç¡®ç‡è®¡ç®—ï¼ˆéªŒè¯é˜¶æ®µï¼‰
                with torch.no_grad():
                    preds = torch.where(out1 > out2, 1, -1)
                    # ä¿æŒä¸è®­ç»ƒé˜¶æ®µç›¸åŒçš„æ ‡ç­¾å¤„ç†æ–¹å¼
                    true_labels = labels.clone()
                    true_labels[true_labels == 0] = 1  # ä¸è®­ç»ƒä¸€è‡´
                    val_correct += (preds.flatten() == true_labels).sum().item()
                    val_total += labels.size(0)

                # ... [åˆ†æ•°è®°å½•ä»£ç ä¸å˜] ...

        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0.0
        val_acc = val_correct / val_total if val_total > 0 else 0.0

        # ä¿å­˜æ¯ä¸ªepochæ¨¡å‹
        epoch_model_path = os.path.join(model_dir, f"epoch_{epoch+1}.pth")
        torch.save(model.state_dict(), epoch_model_path)

        # æ›´æ–°æœ€ä½³æ¨¡å‹æŒ‡æ ‡
        if val_acc > best_metrics['val_acc'] or \
           (val_acc == best_metrics['val_acc'] and avg_val_loss < best_metrics['val_loss']):
            best_metrics.update({
                'epoch': epoch+1,
                'val_loss': avg_val_loss,
                'val_acc': val_acc,
                'train_acc': train_acc
            })
            # ä¿å­˜æœ€ä½³æ¨¡å‹å‰¯æœ¬
            torch.save(model.state_dict(), os.path.join(model_dir, "best_model.pth"))

        # è®°å½•æ—¥å¿—
        with open(log_file, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                epoch+1, 
                running_loss/len(train_loader) if len(train_loader) > 0 else 0.0,
                avg_val_loss,
                train_acc,
                val_acc,
                time.time()-epoch_start,
                epoch_model_path
            ])

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯ï¼ˆæ–°å¢å‡†ç¡®ç‡æ˜¾ç¤ºï¼‰
        epoch_time = time.time() - epoch_start
        print(f"Epoch {epoch+1}/10 | "
              f"Train Loss: {running_loss/len(train_loader):.4f} | "
              f"Val Loss: {avg_val_loss:.4f} | "
              f"Train Acc: {train_acc:.2%} | "  # æ–°å¢
              f"Val Acc: {val_acc:.2%} | "     # æ–°å¢
              f"Time: {timedelta(seconds=int(epoch_time))}")

    # æœ€ç»ˆè¾“å‡ºæœ€ä½³æ¨¡å‹ä¿¡æ¯
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹ï¼šEpoch {best_metrics['epoch']}")
    print(f"â— éªŒè¯é›†æŸå¤±: {best_metrics['val_loss']:.4f}")
    print(f"â— è®­ç»ƒå‡†ç¡®ç‡: {best_metrics['train_acc']:.2%}")
    print(f"â— éªŒè¯å‡†ç¡®ç‡: {best_metrics['val_acc']:.2%}")
    print(f"â— æ¨¡å‹è·¯å¾„: {os.path.join(model_dir, 'best_model.pth')}")

    # æœ€ç»ˆå¤„ç†
    total_time = timedelta(seconds=int(time.time()-total_start))
    print(f"\nè®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_time}")
    
    cursor.close()
    conn.close()
    
    with open(scores_save_path, 'w') as f:
        json.dump({'train_scores': train_scores, 'val_scores': val_scores}, f)
        print(f"è¯„åˆ†å·²ä¿å­˜åˆ° {scores_save_path}")